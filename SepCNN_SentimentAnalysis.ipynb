{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SepCNN_SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRuA4YImbSWm",
        "colab_type": "text"
      },
      "source": [
        "Accessing Sentiment140 Dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQSF9Wr2TSAL",
        "colab_type": "code",
        "outputId": "72485acc-d4db-47a0-9a37-82c1551976ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "# DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "# Matplot\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Scikit-learn\n",
        "import sklearn as sk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "#from sklearn.manifold import TSNE\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Keras\n",
        "import keras\n",
        "#from keras.preprocessing.text import Tokenizer\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "#from keras import utils\n",
        "#from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "#from nltk.corpus import stopwords\n",
        "#from  nltk.stem import SnowballStemmer\n",
        "\n",
        "# Word2vec\n",
        "import gensim\n",
        "\n",
        "# Utility\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import Counter\n",
        "import logging\n",
        "import time\n",
        "import pickle\n",
        "import itertools\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofEkjHMkhwWK",
        "colab_type": "code",
        "outputId": "e4ff95d6-3820-44f0-e58f-60a10cd2b0ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "#mount the drive from google drive, where dataset is kept\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(os.listdir(\"../content/drive/My Drive/sentiment140\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "['training.1600000.processed.noemoticon.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyhKt39ITbRQ",
        "colab_type": "code",
        "outputId": "5fd38e9a-fefb-46fb-ad19-7c0a6d9882f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNTPomF5hLrU",
        "colab_type": "text"
      },
      "source": [
        "Import dataset and create dataframe in runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Es8Dx-kJdnJl",
        "colab": {}
      },
      "source": [
        "columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "dataset = pd.read_csv('../content/drive/My Drive/sentiment140/training.1600000.processed.noemoticon.csv', encoding = 'latin-1', names = columns, engine='python')\n",
        "train_set, test_set = sk.model_selection.train_test_split(dataset, test_size = 0.2, random_state = 457) #shuffle using random-state seed 457"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyRxxe2JhtHi",
        "colab_type": "code",
        "outputId": "600f859f-bc83-499f-c54e-b194430746e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "print('Training Dataset Size: ', len(train_set))\n",
        "print('Test Dataset Size: ', len(test_set))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0831001c5e4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training Dataset Size: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Dataset Size: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CZoYMB4nBWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "decode_map = {0: \"NEGATIVE\", 2: \"NEUTRAL\", 4: \"POSITIVE\"}\n",
        "def decode_sentiment(label):\n",
        "    return decode_map[int(label)]\n",
        "train_set.target = train_set.target.apply(lambda x: decode_sentiment(x))\n",
        "test_set.target = test_set.target.apply(lambda x: decode_sentiment(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-p-DJIOn2gA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pre-process - Sentiment Decoded DataFrames for Training and Test\n",
        "train_set.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5jBNFsUGqSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL79-b-1n-v9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
        "stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
        "def preprocess(text, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = re.sub('@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "train_set.text = train_set.text.apply(lambda x: preprocess(x))\n",
        "test_set.text = test_set.text.apply(lambda x: preprocess(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlEM3jblqwf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Post-processed - Sentiment Decoded and Text Processed DataFrames\n",
        "train_set.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XPdPZ6dGu54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gW0OMzarNAf",
        "colab_type": "text"
      },
      "source": [
        "Word2Vec - Generate word embeddings for embedding layer of LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tABAXeTErKOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "documents = [_text.split() for _text in train_set.text] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4Qo77iPrWtn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(documents))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUjufknDraN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v_model = gensim.models.word2vec.Word2Vec(size=300, \n",
        "                                            window=5, \n",
        "                                            min_count=10, \n",
        "                                            workers=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfB-sWF3tinc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v_model.build_vocab(documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAh2LBrTtlRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = w2v_model.wv.vocab.keys()\n",
        "vocab_size = len(words)\n",
        "print(\"Vocab size\", vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-1DixKptrU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "w2v_model.train(documents, total_examples=len(documents), epochs=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni9YhL-Rt6WD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Verify word2vec\n",
        "w2v_model.most_similar(\"hello\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WYdqaKgIi01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v_model.most_similar(\"world\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y7uWuV6gcOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v_model.most_similar(\"love\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRqzCEn0vgNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(train_set.text)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Total words\", vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8Ilvhx2hkDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.texts_to_sequences(train_set.text)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UUwFYqG_h_Tz",
        "colab": {}
      },
      "source": [
        "tokenizer.texts_to_sequences(train_set.text)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRBk1BZkvpeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(train_set.text), maxlen=300) #Standardize Text input for model training\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(test_set.text), maxlen=300) #Standardize Text input for test evaluation\n",
        "print('x_train', x_train.shape)\n",
        "print('x_test', x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D4--td4x_M7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = train_set.target.unique().tolist()\n",
        "labels.append('NEUTRAL')\n",
        "labels\n",
        "\n",
        "#Encode Labels\n",
        "encoder = sk.preprocessing.LabelEncoder()\n",
        "encoder.fit(train_set.target.tolist())\n",
        "\n",
        "y_train = encoder.transform(train_set.target.tolist())\n",
        "y_test = encoder.transform(test_set.target.tolist())\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "print(\"y_train\",y_train.shape)\n",
        "print(\"y_test\",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7pKvQlh0azF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"x_train\", x_train.shape)\n",
        "print(\"y_train\", y_train.shape)\n",
        "print()\n",
        "print(\"x_test\", x_test.shape)\n",
        "print(\"y_test\", y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP82_ytZ0o4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the embedding layer\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if word in w2v_model.wv:\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGzQja0B1MHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_layer = keras.layers.Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf6UfyZS1YJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import Dropout\n",
        "from tensorflow.python.keras.layers import Embedding\n",
        "from tensorflow.python.keras.layers import SeparableConv1D\n",
        "from tensorflow.python.keras.layers import MaxPooling1D\n",
        "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
        "\n",
        "kernel_size = 5\n",
        "dropout_rate = 0.2\n",
        "filters = 3\n",
        "pool_size = 3\n",
        "\n",
        "# Create the Separable CNN model\n",
        "model = keras.models.Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Dropout(rate=dropout_rate))\n",
        "model.add(SeparableConv1D(filters=filters,\n",
        "                              kernel_size=kernel_size,\n",
        "                              activation='relu',\n",
        "                              bias_initializer='random_uniform',\n",
        "                              depthwise_initializer='random_uniform',\n",
        "                              padding='same'))\n",
        "model.add(SeparableConv1D(filters=filters,\n",
        "                              kernel_size=kernel_size,\n",
        "                              activation='relu',\n",
        "                              bias_initializer='random_uniform',\n",
        "                              depthwise_initializer='random_uniform',\n",
        "                              padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=pool_size))\n",
        "\n",
        "model.add(SeparableConv1D(filters=filters * 2,\n",
        "                          kernel_size=kernel_size,\n",
        "                          activation='relu',\n",
        "                          bias_initializer='random_uniform',\n",
        "                          depthwise_initializer='random_uniform',\n",
        "                          padding='same'))\n",
        "model.add(SeparableConv1D(filters=filters * 2,\n",
        "                          kernel_size=kernel_size,\n",
        "                          activation='relu',\n",
        "                          bias_initializer='random_uniform',\n",
        "                          depthwise_initializer='random_uniform',\n",
        "                          padding='same'))\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Dropout(rate=dropout_rate))\n",
        "model.add(Dense(2, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US9m3CF-9XLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "keras.utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "files.download('model_plot.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUUSfpUh14dh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=\"adam\",\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn_iEkxh1-xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks = [ keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
        "              keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-DMR74JfkEw_",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=1024,\n",
        "                    epochs=32,\n",
        "                    validation_split=0.1,\n",
        "                    verbose=1,\n",
        "                    callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoV05BdgxRAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "score = model.evaluate(x_test, y_test, batch_size=512)\n",
        "print()\n",
        "print(\"ACCURACY:\",score[1])\n",
        "print(\"LOSS:\",score[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3BQORGd2TcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        " \n",
        "epochs = range(len(acc))\n",
        " \n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training & Validation Accuracy')\n",
        "plt.legend()\n",
        " \n",
        "plt.figure()\n",
        " \n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.title('Training & Validation loss')\n",
        "plt.legend()\n",
        " \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq1UTFR12oCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "def decode_sentiment(score, include_neutral=False):\n",
        "    if include_neutral:        \n",
        "        label = NEUTRAL\n",
        "        if score <= 0.4:\n",
        "            label = NEGATIVE\n",
        "        elif score >= 0.7:\n",
        "            label = POSITIVE\n",
        "\n",
        "        return label\n",
        "    else:\n",
        "        return NEGATIVE if score < 0.5 else POSITIVE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmeQV88oTjv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(text, include_neutral=False):\n",
        "    start_at = time.time()\n",
        "    # Tokenize text\n",
        "    x_test = keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=300)\n",
        "    # Predict\n",
        "    score = model.predict([x_test])[0]\n",
        "    # Decode sentiment\n",
        "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
        "\n",
        "    return {\"label\": label, \"score\": float(score),\n",
        "       \"elapsed_time\": time.time()-start_at}  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLONntU8TqMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(\"I hate how much I love the music\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th3wGA1CUZnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('This ipad is not terrible')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIj05pKlUi_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict('I dont know how to feel about this movie. Its good at some points and bad at others. I think I dont like it')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}